{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cce54d",
   "metadata": {},
   "source": [
    "# üòä Facial Emotion Recognition with FER2013\n",
    "## Training High-Accuracy CNN Model using Transfer Learning\n",
    "\n",
    "**What this notebook does:**\n",
    "- Trains a deep learning model to detect 7 emotions from facial images\n",
    "- Uses EfficientNet (transfer learning) for higher accuracy\n",
    "- Saves the trained model as `face_emotionModel.h5`\n",
    "- You can download the model and use it in your Flask app\n",
    "\n",
    "**Emotions detected:** Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Instructions for Beginners:\n",
    "1. **Enable GPU:** Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU` or `L4 GPU`\n",
    "2. **Run cells in order:** Click the play button (‚ñ∂Ô∏è) on each cell from top to bottom\n",
    "3. **Wait for training:** Training will take 30-60 minutes depending on GPU\n",
    "4. **Download model:** At the end, you'll download `face_emotionModel.h5`\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae193278",
   "metadata": {},
   "source": [
    "## üîß Step 1: Setup and Installation\n",
    "\n",
    "Installing required packages and mounting Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.15.0\n",
    "!pip install -q kaggle\n",
    "!pip install -q efficientnet\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"\\nüñ•Ô∏è  GPU Available: {tf.test.is_gpu_available()}\")\n",
    "print(f\"GPU Device: {tf.test.gpu_device_name()}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61609e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to save model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a folder for this project\n",
    "!mkdir -p /content/drive/MyDrive/FER2013_Models\n",
    "print(\"‚úÖ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d432c68",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Download and Prepare Dataset\n",
    "\n",
    "We'll use the FER2013 dataset from Kaggle.\n",
    "\n",
    "### Option A: Using Kaggle API (Recommended)\n",
    "\n",
    "**How to get your Kaggle API key:**\n",
    "1. Go to https://www.kaggle.com/\n",
    "2. Sign in to your account\n",
    "3. Click on your profile picture ‚Üí `Settings`\n",
    "4. Scroll to `API` section ‚Üí Click `Create New Token`\n",
    "5. A file `kaggle.json` will download\n",
    "6. Upload it in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Please upload your kaggle.json file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"‚úÖ Kaggle API configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FER2013 dataset from Kaggle\n",
    "print(\"‚è¨ Downloading FER2013 dataset...\")\n",
    "!kaggle datasets download -d msambare/fer2013\n",
    "\n",
    "# Unzip the dataset\n",
    "print(\"üìÇ Extracting dataset...\")\n",
    "!unzip -q fer2013.zip -d /content/fer2013\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded and extracted!\")\n",
    "print(\"\\nüìä Dataset structure:\")\n",
    "!ls -la /content/fer2013/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041d691",
   "metadata": {},
   "source": [
    "### Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set dataset paths\n",
    "TRAIN_DIR = '/content/fer2013/train'\n",
    "TEST_DIR = '/content/fer2013/test'\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR):\n",
    "    print(\"‚úÖ Dataset found!\\n\")\n",
    "    \n",
    "    # Count images per emotion\n",
    "    emotions = os.listdir(TRAIN_DIR)\n",
    "    print(\"Training data distribution:\")\n",
    "    for emotion in sorted(emotions):\n",
    "        count = len(os.listdir(os.path.join(TRAIN_DIR, emotion)))\n",
    "        print(f\"  {emotion}: {count} images\")\n",
    "    \n",
    "    print(f\"\\nTotal training images: {sum([len(os.listdir(os.path.join(TRAIN_DIR, e))) for e in emotions])}\")\n",
    "    print(f\"Total test images: {sum([len(os.listdir(os.path.join(TEST_DIR, e))) for e in emotions])}\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not found. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b671d",
   "metadata": {},
   "source": [
    "## üé® Step 3: Data Preprocessing and Augmentation\n",
    "\n",
    "Preparing images and creating data generators with augmentation for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612de2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Image parameters\n",
    "IMG_SIZE = 224  # EfficientNet uses 224x224\n",
    "BATCH_SIZE = 32  # Reduced for Colab memory\n",
    "EPOCHS = 30\n",
    "\n",
    "print(\"üìä Data Preprocessing Settings:\")\n",
    "print(f\"  Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "\n",
    "# Data augmentation for training (helps improve accuracy)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,           # Normalize to [0, 1]\n",
    "    rotation_range=15,           # Rotate images randomly\n",
    "    width_shift_range=0.15,      # Shift horizontally\n",
    "    height_shift_range=0.15,     # Shift vertically\n",
    "    shear_range=0.15,            # Shear transformation\n",
    "    zoom_range=0.15,             # Zoom in/out\n",
    "    horizontal_flip=True,        # Flip horizontally\n",
    "    fill_mode='nearest',         # Fill missing pixels\n",
    "    validation_split=0.2         # Use 20% for validation\n",
    ")\n",
    "\n",
    "# Only rescaling for test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "print(\"\\n‚úÖ Data augmentation configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a778121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "print(\"üìÇ Creating data generators...\\n\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='rgb',  # EfficientNet uses RGB\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "print(f\"\\nüìã Emotion Classes: {class_labels}\")\n",
    "print(f\"\\n‚úÖ Training samples: {train_generator.samples}\")\n",
    "print(f\"‚úÖ Validation samples: {val_generator.samples}\")\n",
    "print(f\"‚úÖ Test samples: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45295a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "print(\"üñºÔ∏è  Sample Training Images:\\n\")\n",
    "\n",
    "sample_images, sample_labels = next(train_generator)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(sample_images[i])\n",
    "    emotion_idx = np.argmax(sample_labels[i])\n",
    "    plt.title(class_labels[emotion_idx])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb5ca2",
   "metadata": {},
   "source": [
    "## üß† Step 4: Build the Model (Transfer Learning with EfficientNet)\n",
    "\n",
    "Using EfficientNet pretrained on ImageNet for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16288af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(\"üèóÔ∏è  Building EfficientNet-based model...\\n\")\n",
    "\n",
    "# Load EfficientNetB0 pretrained on ImageNet\n",
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(7, activation='softmax')  # 7 emotions\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model built successfully!\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbf1a1",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Train the Model\n",
    "\n",
    "Training in two phases:\n",
    "1. **Phase 1:** Train with frozen base (faster)\n",
    "2. **Phase 2:** Fine-tune with unfrozen base (higher accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17288a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '/content/best_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=4,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Training callbacks configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16233fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train with frozen base\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PHASE 1: Training with frozen base model\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tune with unfrozen base\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PHASE 2: Fine-tuning with unfrozen base model\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 training complete!\")\n",
    "print(\"\\nüéâ MODEL TRAINING FINISHED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5c587",
   "metadata": {},
   "source": [
    "## üìä Step 6: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine both phases\n",
    "acc = history_phase1.history['accuracy'] + history_phase2.history['accuracy']\n",
    "val_acc = history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']\n",
    "loss = history_phase1.history['loss'] + history_phase2.history['loss']\n",
    "val_loss = history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.axvline(x=15, color='r', linestyle='--', label='Fine-tuning starts')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.axvline(x=15, color='r', linestyle='--', label='Fine-tuning starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nüìä Evaluating on test set...\\n\")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nüîÆ Generating predictions for confusion matrix...\")\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(\"‚úÖ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, \n",
    "            yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca466fbe",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb962156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "print(\"üíæ Saving model to Google Drive...\\n\")\n",
    "\n",
    "model.save('/content/drive/MyDrive/FER2013_Models/face_emotionModel.h5')\n",
    "print(\"‚úÖ Model saved to Google Drive: /content/drive/MyDrive/FER2013_Models/face_emotionModel.h5\")\n",
    "\n",
    "# Also save to local Colab for download\n",
    "model.save('/content/face_emotionModel.h5')\n",
    "print(\"‚úÖ Model saved to local Colab: /content/face_emotionModel.h5\")\n",
    "\n",
    "print(\"\\nüéâ MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"\\nYou can now download it using the cell below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model to your computer\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading face_emotionModel.h5...\\n\")\n",
    "files.download('/content/face_emotionModel.h5')\n",
    "\n",
    "print(\"\\n‚úÖ Download started! Check your browser's download folder.\")\n",
    "print(\"\\nüìå Next Steps:\")\n",
    "print(\"   1. Move the downloaded file to your FACE_DETECTION folder\")\n",
    "print(\"   2. Run your Flask app: python3 app.py\")\n",
    "print(\"   3. Open http://localhost:5000 in your browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc77d14",
   "metadata": {},
   "source": [
    "## üîÆ Step 8: Test the Model with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict emotion from uploaded image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from google.colab import files\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "EMOTION_RESPONSES = {\n",
    "    'angry': \"You look angry. Take a deep breath! üò§\",\n",
    "    'disgust': \"You seem disgusted. What's bothering you? ü§¢\",\n",
    "    'fear': \"You appear fearful. Don't worry! üò®\",\n",
    "    'happy': \"You're smiling! Keep it up! üòä\",\n",
    "    'sad': \"You are frowning. Why are you sad? üò¢\",\n",
    "    'surprise': \"You look surprised! üòÆ\",\n",
    "    'neutral': \"You have a neutral expression. üòê\"\n",
    "}\n",
    "\n",
    "def predict_emotion(img_path):\n",
    "    # Load and preprocess image\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    emotion_idx = np.argmax(predictions[0])\n",
    "    emotion = class_labels[emotion_idx]\n",
    "    confidence = predictions[0][emotion_idx]\n",
    "    \n",
    "    # Display result\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted: {emotion.upper()} ({confidence*100:.1f}%)\\n{EMOTION_RESPONSES[emotion]}\", \n",
    "              fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return emotion, confidence\n",
    "\n",
    "print(\"‚úÖ Prediction function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and test an image\n",
    "print(\"üì§ Upload an image to test emotion detection:\\n\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nüîÆ Analyzing {filename}...\\n\")\n",
    "    emotion, confidence = predict_emotion(filename)\n",
    "    print(f\"\\n‚úÖ Emotion: {emotion.upper()}\")\n",
    "    print(f\"‚úÖ Confidence: {confidence*100:.2f}%\")\n",
    "    print(f\"‚úÖ Message: {EMOTION_RESPONSES[emotion]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da1358",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Tips to Improve Accuracy\n",
    "\n",
    "### Current Model Performance\n",
    "Your model should achieve **60-70% accuracy** with the current setup.\n",
    "\n",
    "### Ways to Increase Accuracy:\n",
    "\n",
    "1. **Face Detection Preprocessing** (Best improvement: +5-10%)\n",
    "   ```python\n",
    "   # Use MTCNN or dlib to detect and crop faces before training\n",
    "   # This removes background noise\n",
    "   ```\n",
    "\n",
    "2. **Larger Model** (+3-7%)\n",
    "   - Use EfficientNetB3 or EfficientNetB4 instead of B0\n",
    "   - Change: `EfficientNetB0` ‚Üí `EfficientNetB3`\n",
    "\n",
    "3. **More Training** (+2-5%)\n",
    "   - Increase epochs: 30 ‚Üí 50\n",
    "   - Train longer with patience\n",
    "\n",
    "4. **Class Balancing** (+2-4%)\n",
    "   ```python\n",
    "   # Add class weights to handle imbalanced data\n",
    "   from sklearn.utils.class_weight import compute_class_weight\n",
    "   class_weights = compute_class_weight('balanced', \n",
    "                                        classes=np.unique(train_generator.classes),\n",
    "                                        y=train_generator.classes)\n",
    "   # Use in model.fit: class_weight=dict(enumerate(class_weights))\n",
    "   ```\n",
    "\n",
    "5. **Ensemble Models** (+3-6%)\n",
    "   - Train multiple models and average predictions\n",
    "   - Combine EfficientNet + ResNet + VGG\n",
    "\n",
    "6. **Test Time Augmentation (TTA)** (+1-3%)\n",
    "   ```python\n",
    "   # Make predictions on augmented versions and average\n",
    "   ```\n",
    "\n",
    "7. **Better Preprocessing**\n",
    "   - Histogram equalization\n",
    "   - Face alignment\n",
    "   - Remove glasses, hats in preprocessing\n",
    "\n",
    "8. **Mix of Datasets**\n",
    "   - Combine FER2013 with RAF-DB or AffectNet\n",
    "   - More diverse training data\n",
    "\n",
    "### Expected Accuracy:\n",
    "- **Basic CNN:** 50-60%\n",
    "- **Current setup (EfficientNet):** 60-70%\n",
    "- **With face detection:** 70-75%\n",
    "- **With all optimizations:** 75-80%\n",
    "- **State-of-the-art (research):** 80-90%\n",
    "\n",
    "**Note:** FER2013 is challenging due to low resolution (48x48) and ambiguous labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9ec29",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Downloaded and prepared the FER2013 dataset\n",
    "- ‚úÖ Built a CNN model with transfer learning (EfficientNet)\n",
    "- ‚úÖ Trained the model with data augmentation\n",
    "- ‚úÖ Evaluated model performance\n",
    "- ‚úÖ Saved the trained model as `face_emotionModel.h5`\n",
    "\n",
    "### üìå Next Steps:\n",
    "\n",
    "1. **Download your model** (done above)\n",
    "2. **Move it to your project:** Place `face_emotionModel.h5` in your `FACE_DETECTION` folder\n",
    "3. **Install remaining packages locally:**\n",
    "   ```bash\n",
    "   pip3 install opencv-python --user\n",
    "   ```\n",
    "4. **Run your Flask app:**\n",
    "   ```bash\n",
    "   cd FACE_DETECTION\n",
    "   python3 app.py\n",
    "   ```\n",
    "5. **Test locally:** Open http://localhost:5000\n",
    "6. **Deploy to Render** (I'll guide you!)\n",
    "\n",
    "### üí¨ Questions?\n",
    "Let me know if you need help with:\n",
    "- Running the Flask app locally\n",
    "- Deploying to Render\n",
    "- Improving model accuracy\n",
    "- Any troubleshooting\n",
    "\n",
    "**You did great! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
